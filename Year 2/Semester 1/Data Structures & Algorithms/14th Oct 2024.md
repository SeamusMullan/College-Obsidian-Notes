#dsa #year2 

## Big O Notation

Main Notes:
- Ignore Constants
- Biggest power of n dominates
- $g(n)$ is used as the Big O Complexity

For a function $f(n)$ we can justify a good description of its big o notation $O(g(n))$ by showing that $g(n)$ is greater than $f(n)$ after a specific value $f(n')$

$3n^{2}+2n +5 \to O(n^2)$ since we only care about the biggest power of n (or an exponential or smth) and we don't need to worry about constants

When answering questions, $c$ is generally a value you can get by adding all the constants in an equation, i.e.
$$
12n\log n+0.05n^2
$$
Has a $c$ value of $12.05$ and an o notation of $O(n^2)$

### Rules / Laws / Formalities (or whatever) for O Notation
$$\begin{array}
\phantom O(k\times f(n)) = O(f(n)) \\
O(f(n)) + O(g(n)) = O(f(n) + g(n)) \\
O(f(n)) \times O(g(n)) = O(f(n) \times g(n))
\end{array}$$

***

tf does this mean for coding tho?


basically

anything that happens once, or just like a few times (outside a loop) is a constant, so dont give two fucks


```
For, Do While, While and other loops
```
Any loop can be considered $O(n)$ making shit simple

nested loops $\to$ higher powers of n

for example

```
For loop one {
	For loop two
	{
		Some Code
	}
}
```

Since we have two for loops, our o notation is $O(n^2)$ 

Note: if our loop only iterates c times, just take it as a constant, but if its looping n times, change our notation by a power of n

If we don't have a variable time for our loop (i.e. it only ever loops 5 times) then we just use $O(1)$

now.. log n is a bit of a bitch, because god forbid maths was easy

so straight from google:

```
If an algorithm is dividing the elements being considered by 2 each iteration, then it likely has a runtime complexity of O(log N)
```

good example of this is binary search