#stats #year2 


Note: simplifyingstats.wordpress.com -> some cool stats website
# Bivariate Discrete Random Variables

Joint behaviour of two or more random variables. For this section we will look at the joint distribution of two discrete random variables.


## Joint Distributions

A family has 3 children. assume that P(boy) = P(girl). What are the possible outcomes?

$\ohm = \text{all possible combinations, i.e. 8 outcomes}$

Now lets say $x$ is the total number of boys, we can have 0, 1, 2 or 3
Now we can say $y$ is a boolean that shows if the first born is female

now we have 2 random variables, with which we can define joint probabilities in a table. i.e. a column for $x$ and a column for $y$ with the related value for the variable

Now we can create a joint probability distribution for $x$ AND $y$

### Joint probability mass functions

we can write a mass probability function for the above statements, formatted like this
$$P(X=x, Y=y)$$

|     | X   | 0   | 1   | 2   | 3   |
| --- | --- | --- | --- | --- | --- |
| Y   | //  | //  | //  | //  | //  |
| 0   | //  | 0   | 1/8 | 2/8 | 1/8 |
| 1   | //  | 1/8 | 2/8 | 1/8 | 0   |

### Marginal probabilities

We can collapse the tables over the rows (basically sum Y=1 and Y=0) to get the marginal probability function for $x$.

We can do the same the other way around (summing across the columns) to get the marginal probability function for $y$.

This can be summarised simply like this. when you collapse along a variable, the other variable's marginal probability function will show up.

***Note:** The sum of the outcomes in marginal probability functions also add up to 1*

## Graphing a joint probability mass function

joint PMFs get graphed in a 3D space, where the y axis represents


#### Joint continuous probability density functions

yeah basically you just get a 3d surface, we dont care much about em.

# Marginal and Conditional Distributions

$$\begin{matrix}
\text{Taking the margin of x to be the sum across y, written as} \\
P(X=x)=\sum\limits_{y} P(X=x, Y=y) \\
P(Y=y)=\sum\limits_{x} P(X=x, Y=y)
\end{matrix}$$

We can also get expected values and variances

$$\begin{matrix}
E[X] = \sum\limits_{x} xP(X=x) \\
Var(X) = E[X]^{2}- \mu^{2}, \text{ where Î¼ is the mean}
\end{matrix}$$

